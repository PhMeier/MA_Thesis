Matplotlib created a temporary config/cache directory at /tmp/matplotlib-1r1wqxmu because the default path (/home/students/meier/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/students/meier/mth_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
212
100
25
63
28
55
80
935
Index(['Unnamed: 0', 'premise', 'hypothesis', 'label'], dtype='object')
1
Axis1:  [0.26785714 0.74658869 0.        ]
Axis 0:  [0.27202073 0.6925859  0.        ]
plus_plus
Accuracy: 25.0
(array([1., 0., 0.]), array([0.25, 0.  , 0.  ]), array([0.4, 0. , 0. ]), array([212,   0,   0]))


plus_minus
Accuracy: 28.000000000000004
(array([1., 0., 0.]), array([0.28, 0.  , 0.  ]), array([0.4375, 0.    , 0.    ]), array([100,   0,   0]))


minus_plus
Accuracy: 0.0
(array([0., 0., 0.]), array([0., 0., 0.]), array([0., 0., 0.]), array([ 0.,  0., 25.]))


neutraL_plus
Accuracy: 92.06349206349206
(array([0., 1.]), array([0.        , 0.92063492]), array([0.        , 0.95867769]), array([ 0, 63]))


neutral_minus
Accuracy: 78.57142857142857
(array([0., 1., 0.]), array([0.        , 0.78571429, 0.        ]), array([0.  , 0.88, 0.  ]), array([ 0, 28,  0]))


minus_neutral
Accuracy: 0.0
(array([0., 0., 0.]), array([0., 0., 0.]), array([0., 0., 0.]), array([ 0.,  0., 55.]))


plus_neutral
Accuracy: 30.0
(array([1., 0.]), array([0.3, 0. ]), array([0.46153846, 0.        ]), array([80,  0]))


neutral_neutral
Accuracy: 73.36898395721924
(array([0., 1., 0.]), array([0.        , 0.73368984, 0.        ]), array([0.        , 0.84639112, 0.        ]), array([  0, 935,   0]))


Index(['Unnamed: 0', 'premise', 'hypothesis', 'label'], dtype='object')
1
[[105 284   3]
 [257 766   3]
 [ 24  56   0]]
Axis1:  [0.26785714 0.74658869 0.        ]
Axis 0:  [0.27202073 0.6925859  0.        ]
Accuracy: 58.14419225634179
Precision: 32.15355401687139
Recall Score: 33.81486122714193
F1 Score: 32.949899599846304
               precision    recall  f1-score   support

   entailment       0.27      0.27      0.27       392
      neutral       0.69      0.75      0.72      1026
contradiction       0.00      0.00      0.00        80

     accuracy                           0.58      1498
    macro avg       0.32      0.34      0.33      1498
 weighted avg       0.55      0.58      0.56      1498

